{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Khai báo thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "PATH_DATASETS = 'data'\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Định nghĩa module nạp dữ liệu\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class FMNISTDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_dir: str = PATH_DATASETS,\n",
    "            batch_size: int = BATCH_SIZE,\n",
    "            num_workers: int = NUM_WORKERS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose({\n",
    "            # convert image or numpy.array (HxWxC) in the range [0,225] to in the range[0,1]\n",
    "            transforms.ToTensor(),\n",
    "            # Normalize a tensor image with mean and standard deviation\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        })\n",
    "\n",
    "        # self.dims is returned when you call dm.size()\n",
    "        # Setting default dims here because we know them.\n",
    "        # Could optionally be assigned dynamically in dm.setup()\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        FashionMNIST(self.data_dir, train=True, download=True)\n",
    "        FashionMNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            feminist_full = FashionMNIST(\n",
    "                self.data_dir, train=True,\n",
    "                transform=self.transform)\n",
    "            self.femnist_train, self.femnist_val = random_split(feminist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.fmnist_test = FashionMNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.femnist_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.femnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.fmnist_test, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Định nghĩa mô hình CGAN\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_shape = 100\n",
    "        self.embedding = nn.Embedding(10, 10)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(110, 256, normalize=False),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, 28 * 28),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # pass the labels into a embedding layer\n",
    "        labels_embedding = self.embedding(y)\n",
    "        # concat the embedded labels and the noise tensor\n",
    "        # z is a tensor of size (batch_size, latent_dim + dim_label_encode)\n",
    "        z = torch.cat([z, labels_embedding], dim=-1)\n",
    "        img = self.model(z)\n",
    "        # model returns three tensors in its forward method: return self.decode(z), mu, logvar\n",
    "        # so this return tensor and shape\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(10, 10)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28 * 28 + 10, 512),\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # pass the labels into a embedding layer\n",
    "        labels_embedding = self.embedding(y)\n",
    "        # concat the embedded labels and the noise tensor\n",
    "        # z is a tensor of size (batch_size, latent_dim + dim_label_encode)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = torch.cat([z, labels_embedding], dim=-1)\n",
    "        img = self.model(z)\n",
    "        # model returns three tensors in its forward method: return self.decode(z), mu, logvar\n",
    "        # so this return tensor and shape\n",
    "        return img\n",
    "\n",
    "\n",
    "class CGAN(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        \"\"\"\n",
    "        Generates an image using the generator\n",
    "        given input noise z and labels y\n",
    "        \"\"\"\n",
    "        return self.generator(z, y)\n",
    "\n",
    "    def generator_step(self, x):\n",
    "        \"\"\"\n",
    "        Training step for generator\n",
    "        1. Sample random noise and labels\n",
    "        2. Pass noise and labels to generator to\n",
    "           generate images\n",
    "        3. Classify generated images using\n",
    "           the discriminator\n",
    "        4. Backprop loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample random noise and labels\n",
    "        z = torch.randn(x.shape[0], 100, device=device)\n",
    "        y = torch.randint(0, 10, size=(x.shape[0],), device=device)\n",
    "\n",
    "        # Generate images\n",
    "        generated_imgs = self(z, y)\n",
    "\n",
    "        # Classify generated image using the discriminator\n",
    "        d_output = torch.squeeze(self.discriminator(generated_imgs, y))\n",
    "\n",
    "        # Backprop loss. We want to maximize the discriminator's\n",
    "        # loss, which is equivalent to minimizing the loss with the true\n",
    "        # labels flipped (i.e. y_true=1 for fake images). We do this\n",
    "        # as PyTorch can only minimize a function instead of maximizing\n",
    "        g_loss = nn.BCELoss()(d_output,\n",
    "                              torch.ones(x.shape[0], device=device))\n",
    "\n",
    "        return g_loss\n",
    "\n",
    "    def discriminator_step(self, x, y):\n",
    "        \"\"\"\n",
    "        Training step for discriminator\n",
    "        1. Get actual images and labels\n",
    "        2. Predict probabilities of actual images and get BCE loss\n",
    "        3. Get fake images from generator\n",
    "        4. Predict probabilities of fake images and get BCE loss\n",
    "        5. Combine loss from both and backprop\n",
    "        \"\"\"\n",
    "\n",
    "        # Real images\n",
    "        d_output = torch.squeeze(self.discriminator(x, y))\n",
    "        loss_real = nn.BCELoss()(d_output,\n",
    "                                 torch.ones(x.shape[0], device=device))\n",
    "\n",
    "        # Fake images\n",
    "        z = torch.randn(x.shape[0], 100, device=device)\n",
    "        y = torch.randint(0, 10, size=(x.shape[0],), device=device)\n",
    "\n",
    "        generated_imgs = self(z, y)\n",
    "        d_output = torch.squeeze(self.discriminator(generated_imgs, y))\n",
    "        loss_fake = nn.BCELoss()(d_output,\n",
    "                                 torch.zeros(x.shape[0], device=device))\n",
    "\n",
    "        return loss_real + loss_fake\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        X, y = batch\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            loss = self.generator_step(X)\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            loss = self.discriminator_step(X, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n",
    "        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n",
    "        return [g_optimizer, d_optimizer], []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Chạy huấn luyện mô hình\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 1.5 M \n",
      "1 | discriminator | Discriminator | 538 K \n",
      "------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.123     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | -1/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36faa5ca5b344d6595a1dc3c09ac43c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = FMNISTDataModule()\n",
    "model = CGAN()\n",
    "dm.setup('fit')\n",
    "mnist_dataloader = dm.train_dataloader()\n",
    "trainer = pl.Trainer(max_epochs=5, gpus=1 if torch.cuda.is_available() else 0, progress_bar_refresh_rate=50)\n",
    "trainer.fit(model, mnist_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "py (py)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}